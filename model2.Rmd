---
title: "MHI2002 Final Report"
author: "Steve Hawley"
date: "April 12, 2019"
output: html_document
editor_options: 
  chunk_output_type: console
---
*Following the tutorial found here: https://medium.com/analytics-vidhya/a-guide-to-machine-learning-in-r-for-beginners-part-5-4c00f2366b90*

#QUESTIONS
1. Does this model actually make sense?
2. OK to add duplicate patients (but different appointments) into model
3. No missing data/outliers in no show data set
4. confirm actual output of model
5. Why is model no better than simple guessing 'NO'




#Preparing the data 

```{r, warning=FALSE}
library(tidyverse)

#load the dataset
noshow <- read.csv("KaggleV2-May-2016.csv")

#review the data structure
str(noshow)

noshow$ScheduledDay <- as.Date(noshow$ScheduledDay)
noshow$AppointmentDay <- as.Date(noshow$AppointmentDay)



#convert the binomial/polynomial INTs into factors (each is basically Y/N)
col_int <- c("Scholarship", "Hipertension", "Diabetes","Alcoholism","Handcap","SMS_received")
noshow[col_int] <- lapply(noshow[col_int], factor)

#Segment the data into hx of Alcoholism vs. no hx of Alcoholism 
#also calculating the difference (in days) between appointment day and schedule day
noshow <- noshow %>% 
  mutate(ddif=difftime(noshow$AppointmentDay, noshow$ScheduledDay, units = "days")) %>% 
  select(PatientId,Gender,Age,ddif,Neighbourhood:No.show) %>% 
  filter(Alcoholism=="1", ddif<80) 

noshow$ddif <- as.numeric(noshow$ddif)

outliers <- boxplot(noshow$ddif)$out
noshow1[which(noshow1$ddif %in% outliers),]
noshow1 <- noshow[-which(noshow$ddif %in% outliers),]
boxplot(noshow$ddif)

#bin table
breaks <- c(0,20,40,60,80,100,120)
bins <- cut(noshow$ddif, breaks, include.lowest = T, right = F)
summary(bins)

#number of different neighborhoods
noshow %>% 
  group_by(Neighbourhood) %>% 
  summarize(
    count = n()
  ) %>% 
  arrange(desc(count))

#remove duplicate IDs (i.e., only want to keep one record per patient and not their entire appt hx)
#noshow <- subset(noshow, !duplicated(noshow[,1]))

#Get a summary data subset
summary(noshow)
head(noshow)

#Removing Handcap since almost all values are 0
noshow <- noshow %>% 
  select(-Handcap)

```
#Descriptives


```{r, warning=FALSE}

noshow %>% 
  ggplot(aes(x=No.show, y=Age))+
  geom_boxplot() +  
  theme(text = element_text(size=20)) +
  xlab("No Show") +
  ylab("Age") 
 

noshow %>% 
  ggplot(aes(x=No.show, y=ddif))+
  geom_boxplot() +  
  theme(text = element_text(size=20)) +
  xlab("No Show") +
  ylab("DateDiff")

noshow %>% 
  ggplot(aes(x=No.show, fill=Gender))+
  geom_bar(position = "fill") +
  xlab("No Show") +
  ylab("proportion") +
  scale_y_continuous(labels = scales::percent_format())

noshow %>% 
  ggplot(aes(x=No.show, fill=SMS_received))+
  geom_bar(position = "fill") +  
  theme(text = element_text(size=20)) +
  xlab("No Show") +
  ylab("proportion") +
  scale_y_continuous(labels = scales::percent_format())

noshow %>% 
  ggplot(aes(x=No.show, fill=Scholarship))+
  geom_bar(position = "fill") +  
  theme(text = element_text(size=20)) +
  xlab("No Show") +
  ylab("proportion") +
  scale_y_continuous(labels = scales::percent_format())

```

#Creating the model

##Baseline Model
We can see that most (2683/3360) people show up for their appointments (i.e., No Show = No). Thus, if the model were to predict "No Show = No"" for every patient, our model would be 79.9% accurate. 

```{r, warning=FALSE, results='markup'}
library(caTools) #for splitting data
library(ROCR) # for ROC curve

#baseline model
bl <- table(noshow$No.show)
bl
bl[1]/(bl[1]+bl[2])*100

#splitting the data
set.seed(88)
split <- sample.split(noshow$No.show, SplitRatio = 0.80)

#create training and test data
noshow_train <- subset(noshow, split == T)
noshow_test <- subset(noshow, split == F)

#write.csv(noshow_train, "noshow_train.csv")

#build the model
noshow_mod <- glm(No.show~Gender+Age+ddif+Scholarship+Hipertension+Diabetes+SMS_received,
             family = binomial,
             data = noshow_train)

summary(noshow_mod)

#making predictions
predict_train <- predict(noshow_mod, type = "response")
summary(predict_train)
tapply(predict_train, noshow_train$No.show, mean)

#determining the threshold to use via ROC curve
ROCRpred <- prediction(predict_train, noshow_train$No.show)
ROCRperf <- performance(ROCRpred, "tpr","fpr")
plot(ROCRperf, colorize=T, print.cutoffs.at=seq(0,1,by=0.1), text.adj=c(-0.2,1.7))

#setting the threshold to >=0.6. This was basically chosen via trial and error looking for the highest accuracy
spec_table <- table(noshow_train$No.show, predict_train >=0.8)
spec_table

#sensitivity
spec_table[4]/(spec_table[2]+spec_table[4])

#specificity
spec_table[1]/(spec_table[1]+spec_table[3])

#accuracy
(spec_table[1]+spec_table[4])/(spec_table[1]+spec_table[2]+spec_table[3]+spec_table[4])
acc_perf <- performance(ROCRpred,"acc")
plot(acc_perf)

#Area under the curve
auc.temp <- performance(ROCRpred,"auc")
auc <- as.numeric(auc.temp@y.values)
auc

```
#Testing the model


```{r}
predict_test <- predict(noshow_mod, type = "response", newdata = noshow_test)

test_tble <- table(noshow_test$No.show,predict_test >=0.7)
test_tble

#sensitivity
test_tble[4]/(test_tble[2]+test_tble[4])

#specificity
test_tble[1]/(test_tble[1]+test_tble[3])

#accuracy
(test_tble[1]+test_tble[4])/(test_tble[1]+test_tble[2]+test_tble[3]+test_tble[4])

```
Thus, this model is approximately 2% more accurate than simply assigning every patient as "No Show = No"

#Simulator
```{r}
gend <- 'M'
age <- 76
ddif <- 0
schol <- as.factor(0)
hiper <- as.factor(0)
diab <- as.factor(0)
sms <- as.factor(1)

predict(noshow_mod, type = "response", newdata = data.frame(Gender=gend,Age=age,ddif=ddif,Scholarship=schol,Hipertension=hiper,Diabetes=diab,SMS_received=sms))


```

#Calibration Curve
```{r}
library(rms)

f1 <- lrm(No.show~Gender+Age+ddif+Scholarship+Hipertension+Diabetes+SMS_received,
             x = T,
          y = T,
             data = noshow_train)
f1
cal <- calibrate(f1)
plot(cal,x = )

df <- do.call(rbind, Map(data.frame, A=ROCRpred@predictions, B=ROCRpred@labels))

write.csv(df,"cal.csv")
```